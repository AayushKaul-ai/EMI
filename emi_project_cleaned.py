# -*- coding: utf-8 -*-
"""EMI_Project_cleaned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JOGCX7Zqoq_Nzl0D8KY7Y_T7PFu_7ZCn
"""

import os,joblib,traceback

# Define the helper function directly in the global scope
def ensure_classification_bundle(auto_retrain=True, training_code_str=""):
    """
    Ensures classification model + feature_names files exist.
    If missing, automatically retra`ins using the provided training_code_str.
    """
    bundle_file = "best_bundle_classification.pkl"
    feat_file = "feature_names_classification.pkl"

    if os.path.exists(bundle_file) and os.path.exists(feat_file):
        print("✔ Classification bundle and feature files found.")
        return True

    if not auto_retrain:
        raise FileNotFoundError(f"❌ Required files missing: {bundle_file} or {feat_file}")

    if not training_code_str:
        raise RuntimeError("❌ `training_code_str` argument is empty. Provide the actual training script as a string to enable auto-retraining.")

    print("⚠ Missing model files — starting AUTO-RETRAIN...")

    # Execute the training code
    try:
        exec(training_code_str, globals(), locals())
    except Exception as e:
        tb = traceback.format_exc()
        raise RuntimeError(f"❌ Auto-retraining failed:\n{tb}")

    # Check again
    if not (os.path.exists(bundle_file) and os.path.exists(feat_file)):
        raise RuntimeError(
            "❌ Training completed but files were not saved.\n"
            "Make sure your training code includes:\n"
            "joblib.dump(bundle, 'best_bundle_classification.pkl')\n"
            "joblib.dump(feature_names_classification, 'feature_names_classification.pkl')"
        )

    print("✔ Auto-retrain completed successfully!")
    return True

# Define the TRAINING_CODE variable separately. This string should contain the *actual*
# Python code that performs the model training and saves the bundle and feature names.
# For now, it's set to a placeholder, as the actual training logic is performed directly
# in other cells. The user would need to consolidate their training logic here if
# they want auto-retraining to function.
TRAINING_CODE = """
# Assuming necessary libraries like sklearn.ensemble, xgboost, joblib, pandas are imported globally
# Assuming X_smote, Y_smote, Xtrain, cat, con, target_col are available in the global scope

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import joblib
import pandas as pd # Needed for Xtrain.columns access if not already imported in the exec scope

# Train the XGBClassifier model, which was identified as 'best_model' in a previous cell
model_classifier = XGBClassifier()
model_classifier.fit(X_smote, Y_smote) # Use X_smote and Y_smote as per current training flow

# Define feature_names_classification from Xtrain.columns
feature_names_classification = list(Xtrain.columns)

# Create the bundle
bundle = {
    "model": model_classifier,
    "preprocessor": None, # Based on the notebook, preprocessor is not used for classification in the bundle
    "feature_names": feature_names_classification, # Use the defined feature_names_classification
    "numeric_features": con,
    "categorical_features": cat,
    "target_col": target_col,
    "mode": "classification"
}

# Save the bundle and feature names
joblib.dump(bundle, "best_bundle_classification.pkl")
joblib.dump(feature_names_classification, "feature_names_classification.pkl")
"""

# Added: Standard imports and helper to normalize column names
import pandas as pd
import numpy as np
def normalize_cols(df):
    """Normalize DataFrame column names: strip, lower, replace spaces with underscore."""
    df = df.copy()
    df.columns = [str(c).strip().lower().replace(' ', '_') for c in df.columns]
    return df

print('Helper normalize_cols and common imports are available.')

# NOTE: I didn't find OneHotEncoder usage in the notebook; if you use it, ensure:
# OneHotEncoder(handle_unknown='ignore', sparse=False)
print('Reminder: if using OneHotEncoder, use handle_unknown="ignore" to avoid unseen-category errors')

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlflow

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression,LinearRegression
from xgboost import XGBClassifier,XGBRegressor
from sklearn.metrics import accuracy_score,f1_score,recall_score,confusion_matrix,r2_score,mean_squared_error,classification_report
import mlflow
import mlflow.sklearn
import streamlit as st
from scipy import sparse
import joblib
import os,math
from collections import Counter

EXPERIMENT_NAME = "EMI Prediction"
mlflow.set_experiment(EXPERIMENT_NAME)

# 1. Load Data
df = pd.read_csv("emi_prediction_dataset.csv", encoding="utf-8")

df.info()

df.isnull().sum()

# Null Value Treatment
for i in df.columns:
    if df[i].dtypes=='object':
      df[i]=df[i].fillna(df[i].mode()[0])
    else:
      df[i]=df[i].fillna(df[i].mean())

df.isnull().sum()

#Classification
target_col = 'emi_eligibility'

# Map target to numerical (Eligible -> 1, Not_Eligible -> 0)
df[target_col] = df[target_col].map({'Eligible': 1, 'Not_Eligible': 0})
df = df.dropna(subset=[target_col])
df[target_col] = df[target_col].astype('int')

# Print value counts to be sure both classes exist
print(df[target_col].value_counts())

# Remove columns that shouldn't be features
drop_cols = ['emi_eligibility']
X = df.drop(drop_cols, axis=1)
y = df[target_col]

if "monthly_salary" in df.columns and "max_monthly_emi" in df.columns:
    # Convert 'monthly_salary' to numeric, handling non-numeric values by coercing to NaN
    df["monthly_salary"] = pd.to_numeric(df["monthly_salary"], errors='coerce')
    # Fill any NaNs created by coercion or existing ones with the mean of the column
    df["monthly_salary"] = df["monthly_salary"].fillna(df["monthly_salary"].mean())
    # Now perform the division with numeric types
    df["emi_to_income_ratio"] = df["max_monthly_emi"] / df["monthly_salary"].replace(0, np.nan)

# Separate cat and con from X
cat=[]
con=[]
for i in X.columns:
  if X[i].dtypes==object:
    cat.append(i)
  else:
    con.append(i)

Xcat=X[cat]
Xcon=X[con]

# Preprocessing
from sklearn.preprocessing import LabelEncoder,StandardScaler

# Feature Encoding
le=LabelEncoder()
for i in Xcat.columns:
    # Convert columns with mixed data types to numeric
    if i in ['age', 'monthly_salary', 'bank_balance']:
        Xcat[i] = pd.to_numeric(Xcat[i], errors='coerce')
        # Fill NaN values after conversion with the mean
        Xcat[i] = Xcat[i].fillna(Xcat[i].mean())
    else:
      Xcat[i]=le.fit_transform(Xcat[i])

# Feature Scaling
ss= StandardScaler()
Xcon=pd.DataFrame(ss.fit_transform(Xcon),columns=con)

X=Xcon.join(Xcat)
X

# Label Encode Y since  it is a categorical output with related categoric outputs.
y=le.fit_transform(y)
y=pd.DataFrame(y,columns=['emi_eligibility'])
y

# Remove outliers and reset the index.
out=[]
for i in Xcon.columns:
  o1=Xcon[(Xcon[i]<-3)|(Xcon[i]>3)].index
  out.extend(o1)
out=list(set(out))

print(out)

X=X.drop(index=out,axis=0)
y=y.drop(index=out,axis=0)

X.shape

y.shape

# Perform train test split
from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)

#  Reduce biasedness of data using SMOTE
from imblearn.over_sampling import SMOTE

# Check for any remaining NaN values in X and y
print("NaNs in X before SMOTE:", X.isnull().sum().sum())
print("NaNs in y before SMOTE:", y.isnull().sum().sum())

# Drop rows with NaN values from both X and y
X_cleaned = X.dropna()
y_cleaned = y.loc[X_cleaned.index] # Ensure y is aligned with cleaned X

# Apply SMOTE to the cleaned data
X_smote, Y_smote = SMOTE().fit_resample(X_cleaned, y_cleaned)

Y_smote.value_counts()

# Create graphs of various columns
from matplotlib.pyplot import plot,show
import seaborn as sns
import matplotlib.pyplot as plt

for i in con:
  sns.boxplot(x=Y_smote['emi_eligibility'],y=X_smote[i].head(100))
  plt.title(f'Boxplot of {i}')
  show()

for i in con:
  sns.histplot(x=X_smote[i].head(100),hue=Y_smote['emi_eligibility'],kde=True)
  plt.title(f'Histogram of {i}')
  show()

for i in cat:
  x=X_smote[i].head(100).value_counts()
  plt.pie(x,labels=x.index,autopct='%1.1f%%')
  plt.title(f'Pie chart of {i}')
  plt.show()

lr=LogisticRegression(random_state =42)
model=lr.fit(X_smote,Y_smote)

# Drop rows with NaN values from Xtest
Xtest_cleaned = Xtest.dropna()
# Ensure Ytest is aligned with cleaned Xtest
Ytest_cleaned = Ytest.loc[Xtest_cleaned.index]

Ypred_train=model.predict(X_smote)
Ypred_test=model.predict(Xtest_cleaned)

feature_names_classification = normalize_cols(X).columns.tolist()
import joblib
preprocessor = None # Ensure preprocessor is explicitly set to None for this bundle
bundle = {
    "model": model,                       # trained model (classifier or regressor)
    "preprocessor": preprocessor,         # fitted ColumnTransformer / pipeline
    "feature_names": list(Xtrain.columns),   # RAW INPUT column names (important!)

    # Optional but very useful metadata
    "numeric_features": con if 'numeric_features' in globals() else [],
    "categorical_features": cat if 'categorical_features' in globals() else [],
    "target_col": target_col if 'target_col' in globals() else None,
    "mode": "classification"
}

joblib.dump(bundle, "model_bundle.pkl")
print("Saved model_bundle.pkl with keys:", bundle.keys())

import joblib,os # Ensure these are imported globally or within the cell

# Ensure the training bundle & feature names exist; will auto-retrain by executing TRAINING_CODE if missing.
try:
    ensure_classification_bundle(auto_retrain=True, training_code_str=TRAINING_CODE)
except Exception as e:
    raise RuntimeError("Could not ensure classification bundle exists: " + str(e))

# Now load safely
bundle = joblib.load("best_bundle_classification.pkl")
preprocessor = bundle["preprocessor"]
model = bundle["model"]

feature_names = joblib.load("feature_names_classification.pkl")
print("Model and feature names loaded successfully.")

print("Training accuracy score is",accuracy_score(Y_smote,Ypred_train))
print("Testing accuracy score is",accuracy_score(Ytest_cleaned,Ypred_test))
print('***')
print("Training confusion matrix is",confusion_matrix(Y_smote,Ypred_train))
print("Testing confusion matrix is",confusion_matrix(Ytest_cleaned,Ypred_test))

# 5. Train and Evaluate
dtc=DecisionTreeClassifier(max_depth=3)
rfc=RandomForestClassifier(n_estimators=10)
xgb=XGBClassifier()

dtc=DecisionTreeClassifier(max_depth =3)
model=dtc.fit(X_smote,Y_smote)

# Ensure Ytest is aligned with cleaned Xtest
Ytest_cleaned = Ytest.loc[Xtest_cleaned.index]

Ypred_train=model.predict(X_smote)
Ypred_test=model.predict(Xtest_cleaned)

print("Training accuracy score is",accuracy_score(Y_smote,Ypred_train))
print("Testing accuracy score is",accuracy_score(Ytest_cleaned,Ypred_test))
print('***')
print("Training confusion matrix is",confusion_matrix(Y_smote,Ypred_train))
print("Testing confusion matrix is",confusion_matrix(Ytest_cleaned,Ypred_test))

rfc=RandomForestClassifier(n_estimators=10)
model=rfc.fit(X_smote,Y_smote)

# Ensure Ytest is aligned with cleaned Xtest
Ytest_cleaned = Ytest.loc[Xtest_cleaned.index]

Ypred_train=model.predict(X_smote)
Ypred_test=model.predict(Xtest_cleaned)

print("Training accuracy score is",accuracy_score(Y_smote,Ypred_train))
print("Testing accuracy score is",accuracy_score(Ytest_cleaned,Ypred_test))
print('***')
print("Training confusion matrix is",confusion_matrix(Y_smote,Ypred_train))
print("Testing confusion matrix is",confusion_matrix(Ytest_cleaned,Ypred_test))

xgb=XGBClassifier()
best_model=xgb.fit(X_smote,Y_smote)

# Ensure Ytest is aligned with cleaned Xtest
Ytest_cleaned = Ytest.loc[Xtest_cleaned.index]

Ypred_train=model.predict(X_smote)
Ypred_test=model.predict(Xtest_cleaned)

print("Training accuracy score is",accuracy_score(Y_smote,Ypred_train))
print("Testing accuracy score is",accuracy_score(Ytest_cleaned,Ypred_test))
print('***')
print("Training confusion matrix is",confusion_matrix(Y_smote,Ypred_train))
print("Testing confusion matrix is",confusion_matrix(Ytest_cleaned,Ypred_test))

"""## **ML** **FLOW**"""

# Config (edit if desired)
EXPERIMENT_NAME = globals().get("experiment_name", "EMI_Classification")
RUN_NAME = "Classification_Training"
BUNDLE_FILENAMES = [
    "best_bundle_classification.pkl",
    "best_bundle.pkl",
    "best_model_bundle.pkl",
    "best_bundle_classification.save",
    "best_model.pkl",
    "best_bundle_classification.joblib",
    "best_model_bundle.joblib"
]
OUT_DIR = "mlflow_artifacts"
os.makedirs(OUT_DIR, exist_ok=True)

def fetch_or_load(name):
    if name in globals():
        return globals()[name]
    # try to find a bundle file and load it
    for fn in BUNDLE_FILENAMES:
        if os.path.exists(fn):
            b = joblib.load(fn)
            # if bundle is dict-like, try to extract requested object
            if isinstance(b, dict) and name in b:
                return b[name]
            # if user saved model directly in file and requested 'best_model'
            if name == "best_model" and (hasattr(b, "predict") or hasattr(b, "predict_proba")):
                return b
    return None

best_model = fetch_or_load("best_model")
preprocessor = fetch_or_load("preprocessor")
X_test = Xtest_cleaned # Use the cleaned Xtest from previous steps
y_test = Ytest_cleaned # Use the cleaned Ytest from previous steps

# If X_test/y_test not found, try to load them from a bundle
if X_test is None or y_test is None:
    for fn in BUNDLE_FILENAMES:
        if os.path.exists(fn):
            b = joblib.load(fn)
            if isinstance(b, dict):
                X_test = X_test or b.get("X_test") or b.get("X_val") or b.get("X_test_trans") or X_test
                y_test = y_test or b.get("y_test") or b.get("y_val") or y_test

# Final sanity checks
if best_model is None:
    raise RuntimeError("best_model not found in notebook globals or in common bundle files. "
                       "Set variable `best_model` to your fitted XGBClassifier or save & load bundle.")

# Ensure experiment set
mlflow.set_experiment(EXPERIMENT_NAME)

# Prepare transformed test matrix
def safe_transform(preproc, X):
    # if preproc exists and is picklable, use it; otherwise assume model accepts raw X
    if preproc is None:
        return X
    # If X is a pandas Series for single row, convert to DataFrame
    try:
        X_trans = preproc.transform(X)
        return X_trans
    except Exception as e:
        # sometimes preprocessor expects a DataFrame with exact columns — ensure column order
        try:
          if isinstance(X, pd.Series):
                X = X.to_frame().T
          X = X.reindex(columns=preproc.feature_names_in_, fill_value=np.nan)
          return preproc.transform(X)
        except Exception:
            # last resort: try fit/transform? (dangerous) => raise helpful error
            raise RuntimeError(f"preprocessor.transform failed: {e}. Ensure `preprocessor` is the trained ColumnTransformer and X_test is a DataFrame with required columns.")

X_test_trans = safe_transform(preprocessor, X_test) if preprocessor is not None else X_test

# Predictions & basic metrics
y_pred = best_model.predict(X_test_trans)
metrics = {"n_test": int(len(y_test))}

# classification metrics
metrics["accuracy"] = float(accuracy_score(y_test, y_pred))
clf_report = classification_report(y_test, y_pred, output_dict=True)
if "weighted avg" in clf_report:
  metrics["recall_weighted"] = float(clf_report["weighted avg"].get("recall", np.nan))
  metrics["f1_weighted"] = float(clf_report["weighted avg"].get("f1-score", np.nan))

# Start ML Flow
with mlflow.start_run(run_name=RUN_NAME):
    # basic params
    mlflow.log_param("model_type", type(best_model).__name__)
    # try to log common xgboost params if present
    for p in ("n_estimators", "max_depth", "learning_rate"):
        if hasattr(best_model, p):
            try:
                mlflow.log_param(p, getattr(best_model, p))
            except Exception:
                pass

# log metrics
mlflow.log_metrics(metrics)

# save classification report text
report_txt = classification_report(y_test, y_pred)
rpt_path = os.path.join(OUT_DIR, "classification_report.txt")
with open(rpt_path, "w") as f:
  f.write(report_txt)
mlflow.log_artifact(rpt_path, artifact_path="reports")

# confusion matrix plot (works for binary+multiclass)
try:
    cm = confusion_matrix(y_test, y_pred)
    from sklearn.metrics import ConfusionMatrixDisplay
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    fig, ax = plt.subplots(figsize=(6,4))
    disp.plot(ax=ax)
    ax.set_title("Confusion Matrix")
    cm_path = os.path.join(OUT_DIR, "confusion_matrix.png")
    fig.savefig(cm_path, bbox_inches="tight")
    plt.close(fig)
    mlflow.log_artifact(cm_path, artifact_path="confusion_matrices")
except Exception as e:
  print("Warning: could not plot/log confusion matrix:", e)

# Save a joblib bundle (preprocessor + model + metadata) and log as artifact
bundle_name = os.path.join(OUT_DIR, "best_bundle_for_inference.pkl")

try:
  joblib.dump({"preprocessor": preprocessor, "model": best_model, "metrics": metrics}, bundle_name)
  mlflow.log_artifact(bundle_name, artifact_path="bundles")
except Exception as e:
  print("Warning: could not save/log bundle:", e)

# Load saved feature list
feature_names = joblib.load("feature_names_classification.pkl")

# --- Added: Placeholder for new data X_new ---
# In a real scenario, X_new would be loaded from an external source or created.
# For demonstration, we'll use a sample from your existing preprocessed X data.
X_new = X.head(5).copy() # Taking the first 5 rows of your processed X as example new data

# Align inference data
X_new = normalize_cols(X_new)
X_new = X_new.reindex(columns=feature_names, fill_value=0)

feature_names_classification = normalize_cols(X).columns.tolist()
bundle = {
    "model": model,
    "preprocessor": preprocessor,
    # human-friendly metadata:
    "feature_names": list(Xtrain.columns),# raw input column names
    "numeric_features": con,
    "categorical_features": cat,
    "target_col": target_col,
    "mode": "classification"
}
joblib.dump(bundle, "best_bundle_classification.pkl")

# === Modified: align X_new to training columns before transform & predict ===
# Normalize columns same as training
X_new = normalize_cols(X_new)

# Load feature names for the model you intend to use
# For classification bundle:
try:
    feature_names = joblib.load("feature_names_classification.pkl")
except Exception as e:
    # fallback to generic feature_names.pkl if present
    try:
        feature_names = joblib.load("feature_names.pkl")
    except Exception as e2:
        raise RuntimeError("Could not load feature_names_classification.pkl or feature_names.pkl. Save feature names at training time.") from e

# Reindex incoming DataFrame to exact training columns; missing columns filled with 0 (or use a different fill if required)
X_new_aligned = X_new.reindex(columns=feature_names, fill_value=0)

# If preprocessor expects numeric dtypes for certain columns, consider dtype coercion here.
# Now transform and predict, using safe_transform to handle None preprocessor
processed = safe_transform(preprocessor, X_new_aligned)
pred = model.predict(processed)

print('Predictions shape:', len(pred))

run_id = mlflow.active_run().info.run_id
print("MLflow run complete. run_id:", run_id)
print("Logged metrics:", metrics)
print("Test distribution:", Counter(y_test))
mlflow.end_run() # Added to explicitly end the previous run

# Save training feature names
feature_names_classification = normalize_cols(X).columns.tolist()
joblib.dump(feature_names_classification, "feature_names_classification.pkl")

from google.colab import files

# Re-create the bundle object for download
bundle = {"preprocessor": preprocessor, "model": best_model, "metrics": metrics}

# Dump the bundle to the specified file
joblib.dump(bundle, "best_bundle_classification.pkl")

# Download the file
files.download("best_bundle_classification.pkl")

"""##REGRESSION MODEL"""

# 3. Choose your regression target
reg_target = 'max_monthly_emi'
RANDOM_STATE = 42
EXPERIMENT = "EMI_Regression"
RUN_NAME = "Regression_Training" # Define RUN_NAME for regression
TEST_SIZE = 0.20

# === Added: Save training feature names for regression ===
feature_names_classification_regression = normalize_cols(X).columns.tolist()
joblib.dump(feature_names_classification_regression, "feature_names_regression.pkl")
print("Saved feature_names_regression.pkl")

EXPERIMENT_NAME = globals().get("experiment_name", "EMI_Regression")
RUN_NAME = "Regresion_Training"
BUNDLE_FILENAMES = [
    "best_regressor_bundle.pkl",
    "best_bundle_regression.pkl",
    "best_bundle.pkl",
    "best_model_bundle.pkl",
    "best_model.pkl",
    "best_regressor_bundle.joblib",
    "best_model_bundle.joblib"
]

os.makedirs("artifacts", exist_ok=True)
mlflow.set_experiment(EXPERIMENT)

# 4. Drop any columns you don't want to use as features
drop_cols = [reg_target] # Add any more columns if needed (e.g. IDs, labels)
X = df.drop(drop_cols, axis=1)
y = df[reg_target]

# Ensure age, monthly_salary, bank_balance are numeric and handle any NaNs they might contain
# This is done here to ensure these columns are correctly typed before splitting into cat/con
for col_name in ['age', 'monthly_salary', 'bank_balance']:
    if col_name in X.columns:
        # Convert to numeric, coercing errors to NaN
        X[col_name] = pd.to_numeric(X[col_name], errors='coerce')
        # Fill any NaNs created by coercion or already existing with the mean
        X[col_name] = X[col_name].fillna(X[col_name].mean())

# Separate cat and con from X
cat=[]
con=[]
for i in X.columns:
  if X[i].dtypes=='object':
    cat.append(i)
  else:
    con.append(i)

Xcat=X[cat]
Xcon=X[con]

# Feature Encoding
le=LabelEncoder()
for i in Xcat.columns:
    # Only label encode truly categorical columns, as age, monthly_salary, bank_balance should already be numeric
    # The previous step in cell 9aAN0GbNWSr6 should have handled the numeric conversion and NaN filling for these.
    if i not in ['age', 'monthly_salary', 'bank_balance']:
      Xcat[i]=le.fit_transform(Xcat[i])
Xcat = Xcat.reset_index(drop=True) # Ensure consistent index

# Feature Scaling
ss= StandardScaler()
Xcon=pd.DataFrame(ss.fit_transform(Xcon),columns=con).reset_index(drop=True) # Ensure consistent index

import numpy as np
import pandas as pd

X=Xcon.join(Xcat)
y = y.reset_index(drop=True)

# Comprehensive NaN and Inf handling for all columns in X
for col in X.columns:
    if X[col].dtype == 'object':
        # For object columns, fill with mode (most frequent value)
        X[col] = X[col].fillna(X[col].mode()[0])
    elif pd.api.types.is_numeric_dtype(X[col]):
        # Replace inf/-inf with NaN first, then fill NaNs with mean
        X[col] = X[col].replace([np.inf, -np.inf], np.nan)
        X[col] = X[col].fillna(X[col].mean())

# Final check for any remaining NaNs/Infs in X (should be 0 now)
print("NaNs in X after final fillna:", X.isnull().sum().sum())
print("Infs in X after final fillna:", np.isinf(X).sum().sum())

X.shape

y.shape

#  Debugging: Check for NaNs and infs before train_test_split
import numpy as np
print("NaNs in X before split:", X.isnull().sum().sum())
print("Infs in X before split:", np.isinf(X).sum().sum())

# Perform train test split
from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)

# 7. Define models
models = {
    'Random Forest': RandomForestRegressor(random_state=42,n_estimators =100,n_jobs =-1,max_depth =10),
    'XGBoost': XGBRegressor(random_state=42)
    }

def mse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))

results = {}

best_r2_score = -float('inf')
best_reg_model = None

mlflow.end_run() # Ensure any previously active run is ended

for name, model in models.items():
    with mlflow.start_run(run_name=f"{RUN_NAME}_{name}"):
        # Debugging: Check for NaNs and Infs in Xtrain and Ytrain right before fitting
        print(f"--- {name} ---")
        model.fit(Xtrain, Ytrain)
        y_pred = model.predict(Xtest)
        # Calculate metrics
        r2 = r2_score(Ytest, y_pred)
        mse_val = mse(Ytest, y_pred)

feature_names_regression = normalize_cols(X).columns.tolist()
if 'preprocessor' not in globals():
    preprocessor = None

# Save everything needed for Streamlit inference
bundle = {
    "model": model,                      # trained model (classifier or regressor)
    "preprocessor": preprocessor,        # fitted ColumnTransformer / pipeline
    "feature_names": list(Xtrain.columns),# RAW INPUT column names (important!)

    # Optional but very useful metadata
    "numeric_features": con, # 'con' list is available and holds numeric feature names
    "categorical_features": cat, # 'cat' list is available and holds categorical feature names
    "target_col": reg_target, # 'reg_target' is defined for regression
    "mode": "regression"
}

joblib.dump(bundle, "best_bundle_regression.pkl")
print("Saved best_bundle_regression.pkl with keys:", bundle.keys())

bundle_reg = joblib.load("best_bundle_regression.pkl")
preprocessor_reg = bundle_reg["preprocessor"]
model_reg = bundle_reg["model"]
feature_names_reg = joblib.load("feature_names_regression.pkl")

# Load training feature names
feature_names = joblib.load("feature_names_regression.pkl")

# Align new data with training feature columns
X_new = normalize_cols(X_new)
X_new = X_new.reindex(columns=feature_names, fill_value=0)

# Log parameters and metrics
mlflow.log_param("model_name", name)
mlflow.log_metrics({"r2_score": r2, "mse": mse_val})

# Log the model with name and input_example
mlflow.sklearn.log_model(sk_model=model, name="model", input_example=Xtrain)

results[name] = {'R2': r2, 'MSE': mse_val}
print(f"{name} - R2 Score: {r2:.4f}, MSE: {mse_val:.4f}")

if r2 > best_r2_score:
            best_r2_score = r2
            best_reg_model = model

# Save the best regression model locally
if best_reg_model is not None:
    joblib.dump(best_reg_model, os.path.join(OUT_DIR, "best_regressor_model.pkl"))
    print(f"\nBest regression model ({type(best_reg_model).__name__}) saved with R2 score: {best_r2_score:.4f}")

# === Added: Save training feature names for regression ===
feature_names_classification = normalize_cols(X).columns.tolist()
joblib.dump(feature_names_classification_regression, "feature_names_regression.pkl")
print("Saved feature_names_regression.pkl")

from google.colab import files

# Re-create the bundle object for download
bundle = {"preprocessor": preprocessor, "model": best_model, "metrics": metrics}

# Dump the bundle to the specified file
joblib.dump(bundle, "best_bundle_regression.pkl")

# Download the file
files.download("best_bundle_regression.pkl")

import chardet, json

path = "emi_project_cleaned.ipynb"

raw = open(path, "rb").read()
enc = chardet.detect(raw)["encoding"]

print("Detected:", enc)

text = raw.decode(enc, errors="replace")   # decode safely
data = json.loads(text)                    # parse notebook JSON

with open("emi_project_cleaned_UTF8.ipynb", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False)

print("Saved cleaned UTF-8 notebook as emi_project_cleaned_UTF8.ipynb")

